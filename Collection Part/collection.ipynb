{"cells":[{"cell_type":"markdown","metadata":{"id":"9Mo0AoFvGgBu"},"source":["# COLLECTION PART"]},{"cell_type":"markdown","metadata":{"id":"HwtP2Z1cGgBw"},"source":["### SET UP THE ENVIRONMENT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Vv0uvpFGgBw"},"outputs":[],"source":["import pandas as pd\n","import re\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","from reddit_utils import init_reddit, safe_api_call\n","import datetime as dt\n","import time"]},{"cell_type":"markdown","metadata":{"id":"aW1WftU3GgBx"},"source":["### PREPROCESS FUNCTION\n","Preprocess the text so that it is uniform and free of unnecessary elements that could distort the analysis.\n","After all transformations have been applied, the text is merged into a single string and returned."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvpI7WVYGgBx"},"outputs":[],"source":["# Download necessary NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Set of stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess(text):\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Removing not printable characters\n","    text = ''.join(filter(lambda x: x in string.printable, text))\n","    # Removing XSLT tags\n","    text = re.sub(r'&lt;/?[a-z]+&gt;', '', text)\n","    text = text.replace(r'&amp;', 'and')\n","    text = text.replace(r'&gt;', '')\n","    # Removing newline, tabs and special reddit words\n","    text = text.replace('\\n', ' ')\n","    text = text.replace('\\t', ' ')\n","    text = text.replace('[deleted]', '').replace('[removed]', '')\n","    # Removing numbers\n","    text = re.sub(r'\\w*\\d+\\w*', '', text)\n","    # Removing URLs\n","    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","    # Removing Punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    # Removing extra spaces\n","    text = re.sub(r'\\s{2,}', \" \", text)\n","\n","    # Handle emojis\n","    emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n","                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n","                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n","                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n","                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n","                               \"]+\", flags=re.UNICODE)\n","    text = emoji_pattern.sub(r'', text)\n","\n","    # Tokenize text\n","    tokens = word_tokenize(text)\n","\n","    # Remove stopwords\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    return ' '.join(tokens)"]},{"cell_type":"markdown","metadata":{"id":"Z2QATQrpGgBy"},"source":["### COLLECT GROUND TRUTH DATA"]},{"cell_type":"markdown","metadata":{"id":"H4Kg5lWBGgBy"},"source":["We establish a connection via init_reddit().\n","\n","Navigate through the highest-rated posts ever of a given subreddit, limiting the data to a specific time range\n","(20 January 2017 to 20 January 2020), the first 3 years of Tump's presidency.\n","\n","Process the content of posts and comments that meet defined length criteria, to collect only\n","potentially meaningful content.\n","\n","Implement a pause between the processing of each post to avoid exceeding the request limits of Reddit's API.\n","Distinguish between pro-Trump and anti-Trump subreddits by assigning corresponding labels to the data\n","collected from each subreddit.\n","\n","Save the data in a CSV file for storage."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSy4NZAmGgBy"},"outputs":[],"source":["def fetch_groundtruth_data(subreddit, label, limit=1000):  # Limit the number of posts to fetch\n","    reddit = init_reddit()\n","\n","    data = []\n","    for submission in safe_api_call(lambda: reddit.subreddit(subreddit).top(time_filter='all', limit=limit)): # Fetch the top rated posts ever\n","        created_time = dt.datetime.fromtimestamp(submission.created_utc)\n","        start_date = dt.datetime(2017, 1, 20)\n","        end_date = dt.datetime(2020, 1, 20)\n","\n","        if not (start_date <= created_time <= end_date):\n","            continue\n","\n","        # Skip AutoModerator posts\n","        if submission.author and submission.author.name == 'AutoModerator':\n","            continue\n","\n","        # Fetch comments\n","        submission.comments.replace_more(limit=10)  # Retrieves all top-level comments by replacing \"MoreComments\" objects\n","        comments = submission.comments.list()\n","\n","        # Process post\n","        if submission.is_self and (len(submission.title.split()) + len(submission.selftext.split()) >= 6):\n","            post_content = submission.title + \" \" + (submission.selftext if submission.selftext else \"\")\n","            data.append({\n","                'id': submission.id,\n","                'author': submission.author.name if submission.author else 'deleted',\n","                'content': preprocess(post_content),\n","                'created': created_time,\n","                'type': 'post',\n","                'label': label,\n","                'subreddit': subreddit\n","            })\n","\n","        # Process comments\n","        for comment in comments:\n","            if len(comment.body.split()) >= 8:  # Filter short comments\n","                data.append({\n","                    'id': comment.id,\n","                    'author': comment.author.name if comment.author else 'deleted',\n","                    'content': preprocess(comment.body),\n","                    'created': dt.datetime.fromtimestamp(comment.created_utc),\n","                    'type': 'comment',\n","                    'label': label,\n","                    'subreddit': subreddit\n","                })\n","\n","        time.sleep(10)  # Sleep between processing each submission to avoid hitting rate limits\n","\n","\n","    return data\n","\n","# Specify the subreddits and labels\n","pro_trump_subreddits = ['AskTrumpSupporters']\n","anti_trump_subreddits = ['MarchAgainstTrump', 'AntiTrumpAlliance']\n","\n","# Fetch data from each subreddit with appropriate labels\n","all_data = []\n","for subreddit in pro_trump_subreddits:\n","    all_data.extend(fetch_groundtruth_data(subreddit, label=1))  # Label 1 for Pro-Trump\n","\n","for subreddit in anti_trump_subreddits:\n","    all_data.extend(fetch_groundtruth_data(subreddit, label=0))  # Label 0 for Anti-Trump\n","\n","# Convert list of dictionaries to DataFrame\n","df = pd.DataFrame(all_data)\n","df['created'] = pd.to_datetime(df['created'])\n","\n","# Save the data to a CSV file\n","df.to_csv('polarized_reddit_posts_and_comments.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"99r3SqwJGgBy"},"source":["### COLLECT COMMENTS OF CHOSEN TOPICS\n","Establish an authorised connection.\n","\n","Set the specific date range.\n","\n","For each topic of interest and its associated subreddits, collect comments from the top rated posts,\n","check that they have been created in the desired time interval, and verify that an author exists.\n","\n","Apply the preprocess function to the comment content.\n","\n","Implement a pause to prevent exceeding the request limits of the Reddit API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrK2a-8-GgBz"},"outputs":[],"source":["def fetch_comments_by_topic(topics, limit=1000): # return up to 1000 posts\n","    reddit = init_reddit()\n","    topic_comments = {}\n","    start_date = dt.datetime(2017, 1, 20)\n","    end_date = dt.datetime(2020, 1, 20)\n","\n","    for topic, subreddits in topics.items():\n","        comments = []\n","        for subreddit in subreddits:\n","            for submission in safe_api_call(lambda: reddit.subreddit(subreddit).top(time_filter='all', limit=limit)): # Fetch the top rated posts ever\n","                submission.comments.replace_more(limit=10)\n","                for comment in submission.comments.list():\n","                    created_time = dt.datetime.fromtimestamp(comment.created_utc)\n","                    # Ensure the comment was created within the specified time range\n","                    if start_date <= created_time <= end_date:\n","                        if comment.author:  # Ensure there is an author object\n","                            comments.append({\n","                                'id': comment.id,\n","                                'author': comment.author.name if comment.author else 'deleted',\n","                                'link_id': comment.link_id,  # submission ID that the comment belongs to\n","                                'parent_id': comment.parent_id,  # ID of the parent comment (prefixed with t1_). If it is a top-level comment, this returns the submission ID instead (prefixed with t3_)\n","                                'content': preprocess(comment.body),\n","                                'created': created_time\n","                            })\n","\n","                time.sleep(10)\n","\n","        topic_comments[topic] = comments\n","    return topic_comments\n","\n","\n","def save_comments_to_csv(topic_comments):\n","    for topic, comments in topic_comments.items():\n","        df = pd.DataFrame(comments)\n","        df.to_csv(f'{topic}_comments.csv', index=False)\n","\n","\n","topics = {\n","    'guncontrol': ['Firearms', 'guncontrol','gunpolitics'],\n","    'politics': ['Conservative', 'democrats', 'Republican'],\n","    'minority': ['racism', 'lgbt', 'askGSM']\n","}\n","\n","topic_comments = fetch_comments_by_topic(topics)\n","save_comments_to_csv(topic_comments)"]}],"metadata":{"kernelspec":{"display_name":"nlpvenv","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.19"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
